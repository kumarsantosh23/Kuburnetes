kubectl get pods
kubectl get nodes
kubectl describe nodes
kubectl describe pods
kubectl get replicaset
kubectl create - replicaset-definition.yml
kubectl apply -f replicaset-defintion.yml
kubectl edit replicaset new-replica-set
kubectl scale --replicas=5 replicaset new-replica-set
kubectl create deployment http-frontend --image=httpd-alpine
kubectl scale deployment http-frontend --replicas=3
kubectl create -f deployment-definition.yml
kubectl apply -f deployment-definition.yml
kubectl describe deployment
kubectl rollout undo deployment/myapp-replicaset
kubectl rollout status deployment/myapp-replicaset
kubectl rollout undo deployment/myapp-replicaset
kubectl create -f service-definition.yml
kubectl delete service voting-service
kubectl get services
kubectl get pods -0 wide
kubectl get nodes -o wide
kubectl get services -o wide
kubectl delete pod
kubectl delete service
kubectl describe service

-----------------------------------------------------------------------------
namespaces commands

kubectl get namespace
kubectl get pods --namespace=finance
kubectl get deployments --namespace=finance
kubectl delete  deployments redis  --namespace=finance
kubectl create -f pod.yml --namespace=finance
kubectl get pods  --namespace=finance
kubectl get pods --all-namespaces
kubectl config set-context $(kubectl config current-context) --namespace=default
kubectl config set-context $(kubectl config current-context) --namespace=dev
kubectl run redis --image=redis --restart=Never --dry-run -o yaml > pod.yml ( to convert it to yaml for pod )
kubectl create  deployment nginx --image=nginx  --dry-run=client -o yaml > d.yaml ( to convert it to yaml for deployment)
kubectl get ep webapp-service ( to see endpoint after service gets created)

---------------------------------------------------------------------------------------------------------------------------------------
IMPERATIVE COMMANDS WHICH WILL BE USEFULL TO CREATE DEFINITION YAML FILES AND RUN THINGS DIRECTLY

kubectl run redis --image=redis:alpine --dry-run=client -o yaml > pod.yml
kubectl apply pod.yml
kubectl create deployment webapp --image=kodekloud/webapp-color --dry-run=client -o yaml > def_correct.yml
kubectl create deployment webapp --image=kodekloud/webapp-color
kubectl scale deployment webapp --replicas=3
kubectl create ns dev-ns
kubectl create deployment redis-deploy --image=redis --namespace=dev-ns
kubectl scale deployment redis-deploy --replicas=2 --namespace=dev-ns
kubectl run httpd --image=httpd:alpine --port=80 --expose
kubectl create service clusterip httpd --tcp=80:80 --dry-run=client -o yaml > servoce4.yml
kubectl expose deployment simple-webapp-deployment --type=NodePort --target-port=8080 --target-port=8080 --name=webapp-service --dry-run -o yaml > svc.yml
kubectl run redis --image=redis --restart=Never --dry-run -o yaml > pod.yml ( to convert it to yaml for pod )

----------------------------------------------------------------------------------------------------------------------------------------------------

Below is the imperative commands to run a deployment of nginx webserver and exposing the ports

kubectl create  deployment nginx --image=nginx  --dry-run=client -o yaml > d.yaml ( to convert it to yaml for deployment)
kubectl scale deployment nginx --replicas=6
Use below version for services of deployment ( using nodeport as type )
kubectl expose deployment webserver --port=80 --name web-service --dry-run=client -o yaml > service.yml
vi service.yml and then change the type: NodePort and add nodePort: 30028 ( nodeport for example)
kubectl apply -f service.yml
To check if the pods are accessable to that ports do below commands
kubectl get ep webservice
kubectl get nodes -o wide
curl http://nodeipaddress: 30028 ( port which we exposed in above )

----------------------------------------------------------------------------------------------------------------------------------------------------
Below are the commands to filter out pods/replicas/services/objects based on selectors

kubectl get pods --selector env=prod
kubectl get replicasets --selector env=prod
kubectl get all --selector env=pod,bu=finance,tier=frontend
kubectl get pods --selector env=pod,bu=finance,tier=frontend

-------------------------------------------------------------------------------------------------------------------------------------------------------
Taints and Tolerations

kubectl taint nodes node01 spray=mortein:NoSchedule ( Existing pods won't terminate but won't allow new pods to be assigned )
kubectl describe nodes master | grep Taint
kubectl taint nodes node01 spray=mortein:NoSchedule-
kubectl taint nodes node01 app=mos:NoExecute ( This will terminate the pods if already existing after applying taint)
kubectl taint nodes node01 app=mos:NoExecute-

-------------------------------------------------------------------------------------------------------------------------------------------------------
Node Affinity

kubectl label  node node01 --color=blue

--------------------------------------------------------------------------------------------------------------------------------------------------------
Resource limits

kubectl get pod elephant -o yaml > ss.yml ( to generate yaml file where pod is already running )
once yaml file get genrated VI into yaml and change the resourse limit

----------------------------------------------------------------------------------------------------------------------------------------------------------
DaemonSets

kubectl get daemonsets --all-namespaces
kubectl get daemonsets --all-namespaces -o wide
To create a Daemonset Yaml file follow this below couple of steps ( Since there is no direct imperative command)
kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml >ss.yml
VI into ss.yml file and change Kind from Deployment to DaemonSet and remove replicas and then save it.
kubectly apply -f ss.yml --namespace=kube-system ( if you want to run it only under namespace or else remove --namespace)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Static Pods
ps -aux | grep kubelet
cat /var/lib/kubelet/config.yaml
CAT config.yml file and check for staticPdPath
To Create a Static pod use below imperative command
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
To delete pod, go to the staticPdPath (/etc/kuburnetes/resources ) and delete the Yaml file
------------------------------------------------------------------------------------------------------------------------------------------------------------
To create custom scheduler and run it as static pod

Copy the existing scheduler.yml ( /etc/kuburentes/resorces ) to some location and edit it accordingly
copy it back to ( /etc/kuburentes/resorces ) so that scheduler will run as static pod
To use modified scheduler, edit your pod file with  ( schedulerName: my-scheduler )
check kubectl get events

-------------------------------------------------------------------------------------------
Metrics
https://github.com/kodekloudhub/kubernetes-metrics-server.git
after cloning create pods by kubectl create -f .
to check metrics
kubectl get nodes
kubectl get pods

------------------------------------------------------------------------------------------------
Application life cycle management
kubectl get deployments
kubectl edit deployment fronted ( changet rolling update to recreate)
Re-create deployment will kill all pods at a time and bring back new pods at a time.
rolling update will bring down few pods at a time and bring up new pods at a time.
kubectl rollout undo deployment/nginx
kubectl rollout status deployment/nginx
kubectl rollout history deployment/nginx
----------------------------------------------------------------------------------------------------
CMD vs ENTRY POINT
Entry point in the docker file refers to run the commands which is mentioned as KEY Value ( JSON) ( ENTRYPOINT ["python, "app.py"] )
Once Pod is up, it will run python app.py
Command in the docker file refers to added argument to Entry point ( CMD ["--clour" "green"])
This above can be updated through defintion yaml file as below, under image
command: ["python", "app.py"]
args: ["--color", "pink"]

ENTRYPOINT ( DOCKER) = command ( Kube defintion file )
CMD ( Docker ) = args ( kube defintion file )

---------------------------------------------------------------------------------------------------------
ConfigMaps
Two ways to create configMaps Imperative and Declartive
Imperative : kubectl create cm webapp_color --from-literal=APP_COLOR=blue
kubectl get cm
kubectl describe cm
Edit the POD Definition yaml file under sepc as
envFrom:
  - configMapRef:
       name: APP_COLOR
Decleartive : Create config Map definition yaml file see config-map.yml
kubectl create -f config-map.yml
---------------------------------------------------------------------------------------------------------
Secrets
Two ways to create configMaps Imperative and Declartive
Imperative:
kubectl get secrets
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
kubectl explain pod --recursive | grep -A8 envFrom ( To get sytnax )
Edit the POD Definition yaml file under sepc as
envFrom:
- secretRef:
    name: db_secret
Decleartive : Create config Map definition yaml file see secret-map.yml
kubectl create -f secret-map.yml
To print out any logs from the containers inside the pod used below command
kubectl -n elastic-stack exec -it app cat /log/app.log
---------------------------------------------------------------------------------------------------------
UPdagrading cluster wit Kubeadm tool for master and worker node

kubectl drain node01 --ignore-daemonsets ( To send the pods from node01 to other nodes)
kubectl uncordon node01 ( Once OS is upgraded bringing back node01 back ol, previously evacuated pods won't come again)
kubectl drain node02 --ignore-daemonsets --force ( Forcefully to delete the pods which doesn't have replicaset)
kubectl cordon node03 ( this will not delete the running pods from nodes but won't schedule new pods)

Use below commands for upgrading cluster ( Kubeadm upgrade -h)
apt install kubadm=1.18.0-00
kubeadm apply upgrade v1.18.0
apt install kublet=1.18.0-00
apt install kubadm=1.18.0-00
kubeadm upgrade node
apt install kublet=1.18.0-00

ETCD Cluster backup and restore

ETCDCTL_API=3 etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 /tmp/snapshot-pre-boot.db
ETCDCTL_API=3 etcdctl snapshot restore -h
ETCDCTL_API=3 etcdctl snapshot restore --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 --data-dir="/var/lib/etcd-frombackup"  --initial-advertise-peer-urls="http://127.0.0.1:2380"  --initial-cluster="master=http://127.0.0.1:2380" --initial-cluster-token="etcd-cluster-v1" --name="master" /tmp/snapshot-pre-boot.db
---------------------------------------------------------------------------------------------------------
kubectl get csr
kubectl certificate approve akshay
kubectl get csr agent-smith -o yaml
kubectl certificate deny agent-smith
cat akshya.cert | base64 ( To encode the cert and place in csr.yml, not recomended to place the cert file directly under yml )

kubectl config --kubeconfig=/root/my-kube-config use-context research
/root/.kube/config

kubectl get roles --all-namespaces
kubectl describe role kube-proxy -n kube-system
kubectl describe rolebinding kube-proxy -n kube-system
kubectl get pods --as dev-user
kubectl create -f role.yml
kubectl create -f role-binding.yml
kubectl delete roles --all -n blue
kubectl delete rolebinding --all -n blue

kubectl get clusterroles | wc -l
kubectl get clusterrolebinding | wc -l
kubectl auth can-i list nodes --as michelle
kubectl auth can-i list persistentvolumes --as michelle

kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com

kubectl exec -it ubuntu-sleeper -- date -s '19 APR 2012 11:14:00'
edit pod-definition yaml under spec condition as below
securityContext:
  runAsuser: 1001
  capabilities:
    add: ["SYS_TIME"]

kubectl get networkpolicy
kubectl get persistentvolume
kubectl get persistentvolumeclaim
kubectl get storageclasses
kubectl describe storageclasses


ip link
ifconfig -a
Kube-scheduler port number 10251
ip r ( Routine table)
etcd port number 2379 ( To listen the control-panel components within cluster) 2380 ( Peer-peer listner if there i another ETCD on another master)
ps -aux | grep kubelet
ls -tlr /opt/cni/bin
ls /etc/cni/net.d/
cat /etc/cni/net.d/10-weave.conflist

To see the Ip-address range of nodes
ip addr
check ens section

to see the Ip-address range for pods
Ip address for pods gets assigned through any of the network plugin, in this case weave network.
this network plugin will run as pods, to see the ip address range of pods do this below
kubectl get -n kube-system logs  weave-net-ncdbb -c weave ( check for ipalloc)

To see the Ip-address range for service
Services will run accross the clusters, unlike pod where it will run only on any of the node
this services gets deployed through kube-apiserver, so since kube-apiserver runs as static pod, go through the yaml file to get range
cat kube-apiserver.yaml ( check for --service-clusterrange )

DNS ( DOMAIN NAME SYSTEM )
cat /etc/hosts
cat /etc/resolve.conf ( mention dns name and Ip of DNs server)
( nameserver 10.10.1.2)

kubectl get ingress --all-namespaces
kubectl edit --namespace app-space ingress ingress-wear-watch
